{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYl1qQXiGc3YZr/rMJAxfj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nodeswithsumit/Deep_Learning/blob/main/Activaton_Functions_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions\n",
        "\n",
        "Activation functions are crucial components in neural networks, determining whether a neuron should be activated or not based on its input. They introduce non-linearity into the model, enabling it to learn complex patterns. Here are some common types of activation functions and how they work:"
      ],
      "metadata": {
        "id": "283k9c3WEEOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. Binary Step Function**\n",
        "- **Description**: Outputs a binary value (0 or 1) based on a threshold.\n",
        "- **Use Case**: Simple models where binary classification is needed.\n",
        "- **Limitation**: Not suitable for multi-class classification and can hinder backpropagation due to zero gradient.\n",
        "\n",
        "### **2. Linear Activation Function**\n",
        "- **Description**: Outputs the input directly without any transformation.\n",
        "- **Use Case**: Simple linear regression models.\n",
        "- **Limitation**: Cannot handle complex patterns and makes the network equivalent to a single-layer model.\n",
        "\n",
        "### **3. Sigmoid Function**\n",
        "- **Description**: Outputs values between 0 and 1, creating an S-shaped curve.\n",
        "- **Use Case**: Models requiring probability outputs.\n",
        "- **Limitation**: Can cause vanishing gradient problems, making training difficult for deep networks.\n",
        "\n",
        "The sigmoid function outputs values between 0 and 1, making it useful for binary classification problems.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return tf.keras.activations.sigmoid(x)\n",
        "\n",
        "# Example usage\n",
        "x = tf.constant([-1.0, 0.0, 1.0])\n",
        "print(sigmoid(x))\n",
        "```\n",
        "\n",
        "### **4. Tanh (Hyperbolic Tangent) Function**\n",
        "- **Description**: Outputs values between -1 and 1, creating an S-shaped curve centered around zero.\n",
        "- **Use Case**: Models where zero-centered outputs are beneficial.\n",
        "- **Limitation**: Also suffers from vanishing gradient issues.\n",
        "\n",
        "The tanh function outputs values between -1 and 1, which can be useful for zero-centered data.\n",
        "\n",
        "```python\n",
        "# Tanh activation function\n",
        "def tanh(x):\n",
        "    return tf.keras.activations.tanh(x)\n",
        "\n",
        "# Example usage\n",
        "x = tf.constant([-1.0, 0.0, 1.0])\n",
        "print(tanh(x))\n",
        "```\n",
        "\n",
        "### **5. ReLU (Rectified Linear Unit)**\n",
        "- **Description**: Outputs the input directly if positive; otherwise, it outputs zero.\n",
        "- **Use Case**: Most commonly used in hidden layers of neural networks due to its simplicity and efficiency.\n",
        "- **Limitation**: Can cause \"dead neurons\" if many inputs are negative.\n",
        "\n",
        "ReLU is the most commonly used activation function in hidden layers of neural networks due to its simplicity and efficiency.\n",
        "\n",
        "```python\n",
        "# ReLU activation function\n",
        "def relu(x):\n",
        "    return tf.keras.activations.relu(x)\n",
        "\n",
        "# Example usage\n",
        "x = tf.constant([-1.0, 0.0, 1.0])\n",
        "print(relu(x))\n",
        "```\n",
        "\n",
        "### **6. Leaky ReLU**\n",
        "- **Description**: Similar to ReLU but allows a small, non-zero gradient when the input is negative.\n",
        "- **Use Case**: Helps mitigate the \"dead neuron\" problem in ReLU.\n",
        "- **Limitation**: The slope for negative values is a hyperparameter that needs tuning.\n",
        "\n",
        "Leaky ReLU allows a small, non-zero gradient when the input is negative, helping to mitigate the \"dead neuron\" problem.\n",
        "\n",
        "```python\n",
        "# Leaky ReLU activation function\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return tf.keras.layers.LeakyReLU(alpha=alpha)(x)\n",
        "\n",
        "# Example usage\n",
        "x = tf.constant([-1.0, 0.0, 1.0])\n",
        "print(leaky_relu(x))\n",
        "```\n",
        "\n",
        "### **7. Softmax Function**\n",
        "- **Description**: Converts logits into probabilities that sum to 100%, used in the output layer for multi-class classification.\n",
        "- **Use Case**: Multi-class classification problems.\n",
        "- **Limitation**: Computationally expensive compared to other activation functions.\n",
        "\n",
        "Softmax is used in the output layer of neural networks for multi-class classification problems, converting logits into probabilities.\n",
        "\n",
        "```python\n",
        "# Softmax activation function\n",
        "def softmax(x):\n",
        "    return tf.keras.activations.softmax(x)\n",
        "\n",
        "# Example usage\n",
        "x = tf.constant([1.0, 2.0, 3.0])\n",
        "print(softmax(x))\n",
        "```\n",
        "\n",
        "Activation functions play a vital role in the performance and training of neural networks by introducing non-linearity, enabling the network to learn and model complex data patterns\n",
        "\n",
        "References Links:\n",
        "* https://www.v7labs.com/blog/neural-networks-activation-functions\n",
        "* https://www.freecodecamp.org/news/activation-functions-in-neural-networks/\n",
        "* https://www.theiotacademy.co/blog/types-of-activation-function-in-neural-network/."
      ],
      "metadata": {
        "id": "kDzr_QWJDe05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Methods"
      ],
      "metadata": {
        "id": "O2xYT3P7EPUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Sigmoid Activation Function**\n",
        "The sigmoid function outputs values between 0 and 1, making it useful for binary classification problems.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return tf.keras.activations.sigmoid(x)\n",
        "\n",
        "# Example usage\n",
        "x = tf.constant([-1.0, 0.0, 1.0])\n",
        "print(sigmoid(x))\n",
        "```\n",
        "\n",
        "### **2. Tanh Activation Function**\n",
        "The tanh function outputs values between -1 and 1, which can be useful for zero-centered data.\n",
        "\n",
        "```python\n",
        "# Tanh activation function\n",
        "def tanh(x):\n",
        "    return tf.keras.activations.tanh(x)\n",
        "\n",
        "# Example usage\n",
        "x = tf.constant([-1.0, 0.0, 1.0])\n",
        "print(tanh(x))\n",
        "```\n",
        "\n",
        "### **3. ReLU (Rectified Linear Unit)**\n",
        "ReLU is the most commonly used activation function in hidden layers of neural networks due to its simplicity and efficiency.\n",
        "\n",
        "```python\n",
        "# ReLU activation function\n",
        "def relu(x):\n",
        "    return tf.keras.activations.relu(x)\n",
        "\n",
        "# Example usage\n",
        "x = tf.constant([-1.0, 0.0, 1.0])\n",
        "print(relu(x))\n",
        "```\n",
        "\n",
        "### **4. Leaky ReLU**\n",
        "Leaky ReLU allows a small, non-zero gradient when the input is negative, helping to mitigate the \"dead neuron\" problem.\n",
        "\n",
        "```python\n",
        "# Leaky ReLU activation function\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return tf.keras.layers.LeakyReLU(alpha=alpha)(x)\n",
        "\n",
        "# Example usage\n",
        "x = tf.constant([-1.0, 0.0, 1.0])\n",
        "print(leaky_relu(x))\n",
        "```\n",
        "\n",
        "### **5. Softmax**\n",
        "Softmax is used in the output layer of neural networks for multi-class classification problems, converting logits into probabilities.\n",
        "\n",
        "```python\n",
        "# Softmax activation function\n",
        "def softmax(x):\n",
        "    return tf.keras.activations.softmax(x)\n",
        "\n",
        "# Example usage\n",
        "x = tf.constant([1.0, 2.0, 3.0])\n",
        "print(softmax(x))\n",
        "```\n",
        "\n",
        "These functions are essential for introducing non-linearity into neural networks, enabling them to learn and model complex patterns[1](https://www.v7labs.com/blog/neural-networks-activation-functions)[2](https://www.freecodecamp.org/news/activation-functions-in-neural-networks/)[3](https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/).\n"
      ],
      "metadata": {
        "id": "HTOOzOPqEaDs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3h-mvMZeDfZh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}